================================================================================
ğŸš€ Wan2.2 Performance Analysis Report (ViT-Optimized)
================================================================================

[System Context]
Device:       NVIDIA L40S (44.52 GB)
Software:     CUDA 12.8 | PyTorch 2.9.1+cu128
Task Config:  ti2v-5B | 1280*704 | 161 Frames | 5 Steps
Optimization: Offload=ON | Precision=FP16/FP32

[Executive Summary]
--------------------------------------------------------------------------------
âœ… Total Latency:       269.38 s
âš¡ Generation Speed:    12.57 s/step (0.08 steps/s)
ğŸ“¹ Video Throughput:    1.03 frames/s (0.93 MPx/s)
ğŸ’¾ Peak Memory:         31.27 GB (70.2% of VRAM)
ğŸŒ¡ï¸  Avg GPU Activity:    85.8% (Compute Score: 23119.7)

[Detailed Pipeline Latency]
--------------------------------------------------------------------------------
1. Initialization     :  88.50 s ( 32.9%)
2. Text Encoding (T5) :  47.49 s ( 17.6%)
3. Denoising (DiT)    :  62.86 s ( 23.3%)
   - First Step       : 12.65 s
   - Avg Step (Stable): 12.57 s
4. VAE Decoding       :  45.60 s ( 16.9%)

[Memory Telemetry]
--------------------------------------------------------------------------------
Allocation Strategy: [Offload Active]
- DiT Weights (Static): 18.70 GB
- DiT Activation Peak : 9.78 GB
- T5 Peak Usage       : 2.63 GB (Dynamic (Offloaded))
- VAE Peak Usage      : 23.54 GB

[Expert Recommendations]
--------------------------------------------------------------------------------
> â„¹ï¸ Offloading is active. This reduces VRAM but may increase latency slightly due to PCI-e transfers.
> â„¹ï¸ Step speed is < 1 step/s. Consider using Flash Attention 2 or FP8 quantization if supported.

[Execution Trace Analysis]
-------------------------------------------------------------------------------------------------------------------------------------------------
Stage Name                | Time (s) |  SM % |   Load | Mem Start |   Mem End |  Peak Mem |     Mem Î” | Comment
-------------------------------------------------------------------------------------------------------------------------------------------------
Model_Initialization      |    88.49 |   0.3 |   25.4 |      0.00 |      2.63 |      2.63 |      2.63 | Sys Init
TI2V_Preprocess           |     0.00 |   0.0 |    0.0 |      2.63 |      2.63 |      2.63 |      0.00 | 
T5_Encoding               |    47.49 |   0.0 |    0.0 |      2.63 |      2.63 |      2.63 |      0.00 | Text Enc
S0_Loading                |     0.01 |   0.0 |    0.0 |     21.34 |     21.34 |     21.34 |      0.00 | Cached/NoOp
S0                        |    12.65 |  98.8 | 1249.3 |     21.34 |     21.48 |     31.12 |      0.14 | Compute Heavy
S1_Loading                |     0.00 |   0.0 |    0.0 |     21.48 |     21.48 |     21.48 |      0.00 | Cached/NoOp
S1                        |    12.47 |  99.1 | 1235.8 |     21.48 |     21.55 |     31.22 |      0.08 | Compute Heavy
S2_Loading                |     0.00 |   0.0 |    0.0 |     21.55 |     21.53 |     21.55 |     -0.03 | Cached/NoOp
S2                        |    12.56 |  99.3 | 1246.9 |     21.53 |     21.56 |     31.27 |      0.03 | Compute Heavy
S3_Loading                |     0.00 |   0.0 |    0.0 |     21.56 |     21.53 |     21.56 |     -0.03 | Cached/NoOp
S3                        |    12.56 | 100.0 | 1255.9 |     21.53 |     21.56 |     31.27 |      0.03 | Compute Heavy
S4_Loading                |     0.00 |   0.0 |    0.0 |     21.56 |     21.53 |     21.56 |     -0.03 | Cached/NoOp
S4                        |    12.62 | 100.0 | 1261.7 |     21.53 |     21.56 |     31.27 |      0.03 | Compute Heavy
VAE_Decoding              |    45.60 |  99.9 | 4555.6 |      2.93 |      4.55 |     23.54 |      1.62 | Decoding
Total_Execution           |   269.21 |  41.1 | 11057.2 |      0.00 |      2.64 |     31.27 |      2.64 | 
-------------------------------------------------------------------------------------------------------------------------------------------------
Legend: SM% = SM Activity | Load = Duration * SM% | Mem values in GB
================================================================================
