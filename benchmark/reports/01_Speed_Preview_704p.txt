================================================================================
ğŸš€ Wan2.2 Performance Analysis Report (ViT-Optimized)
================================================================================

[System Context]
Device:       NVIDIA L40S (44.4 GB)
Software:     CUDA 12.4 | PyTorch 2.5.1+cu124
Task Config:  ti2v-5B | 1280*704 | 161 Frames | 5 Steps
Optimization: Offload=ON | Precision=FP16/FP32

[Executive Summary]
--------------------------------------------------------------------------------
âœ… Total Latency:       344.73 s
âš¡ Generation Speed:    12.87 s/step (0.08 steps/s)
ğŸ“¹ Video Throughput:    1.09 frames/s (0.98 MPx/s)
ğŸ’¾ Peak Memory:         31.27 GB (70.4% of VRAM)
ğŸŒ¡ï¸  Avg GPU Activity:    0.0% (Compute Score: 0.0)

[Detailed Pipeline Latency]
--------------------------------------------------------------------------------
1. Initialization     : 119.48 s ( 34.7%)
2. Text Encoding (T5) :  37.68 s ( 10.9%)
3. Denoising (DiT)    :  64.35 s ( 18.7%)
   - First Step       : 13.68 s
   - Avg Step (Stable): 12.87 s
4. VAE Decoding       :  45.80 s ( 13.3%)

[Memory Telemetry]
--------------------------------------------------------------------------------
Allocation Strategy: [Offload Active]
- DiT Weights (Static): 18.70 GB
- DiT Activation Peak : 0.00 GB
- T5 Peak Usage       : 2.63 GB (Dynamic (Offloaded))
- VAE Peak Usage      : 23.54 GB

[Expert Recommendations]
--------------------------------------------------------------------------------
> â„¹ï¸ Offloading is active. This reduces VRAM but may increase latency slightly due to PCI-e transfers.
> â„¹ï¸ Step speed is < 1 step/s. Consider using Flash Attention 2 or FP8 quantization if supported.

[Execution Trace Analysis]
-------------------------------------------------------------------------------------------------------------------------------------------------
Stage Name                | Time (s) |  SM % |   Load | Mem Start |   Mem End |  Peak Mem |     Mem Î” | Comment
-------------------------------------------------------------------------------------------------------------------------------------------------
Model_Initialization      |   119.45 |   0.1 |    0.0 |      0.00 |      2.63 |      2.63 |      2.63 | Sys Init
TI2V_Preprocess           |     0.00 |   0.0 |    0.0 |      2.63 |      2.63 |      2.63 |      0.00 | 
T5_Encoding               |    37.68 |   0.0 |    0.0 |      2.63 |      2.63 |      2.63 |      0.00 | Text Enc
S0_Loading                |     0.03 |   0.0 |    0.0 |     21.34 |     21.34 |     21.34 |      0.00 | Cached/NoOp
S0                        |    13.68 |  88.9 |    0.0 |     21.34 |     21.48 |     31.11 |      0.14 | Compute Heavy
S1_Loading                |     0.00 |   0.0 |    0.0 |     21.48 |     21.48 |     21.48 |      0.00 | Cached/NoOp
S1                        |    12.50 |  99.8 |    0.0 |     21.48 |     21.55 |     31.22 |      0.08 | Compute Heavy
S2_Loading                |     0.00 |   0.0 |    0.0 |     21.55 |     21.53 |     21.55 |     -0.03 | Cached/NoOp
S2                        |    12.83 |  99.2 |    0.0 |     21.53 |     21.55 |     31.27 |      0.03 | Compute Heavy
S3_Loading                |     0.00 |   0.0 |    0.0 |     21.55 |     21.53 |     21.55 |     -0.03 | Cached/NoOp
S3                        |    12.64 |  98.3 |    0.0 |     21.53 |     21.55 |     31.27 |      0.03 | Compute Heavy
S4_Loading                |     0.00 |   0.0 |    0.0 |     21.55 |     21.53 |     21.55 |     -0.03 | Cached/NoOp
S4                        |    12.70 |  99.9 |    0.0 |     21.53 |     21.55 |     31.27 |      0.03 | Compute Heavy
VAE_Decoding              |    45.80 | 100.0 |    0.0 |      2.93 |      4.55 |     23.54 |      1.62 | Decoding
Total_Execution           |   339.17 |  33.5 |    0.0 |      0.00 |      2.64 |     31.27 |      2.64 | 
-------------------------------------------------------------------------------------------------------------------------------------------------
Legend: SM% = SM Activity | Load = Duration * SM% | Mem values in GB
================================================================================
